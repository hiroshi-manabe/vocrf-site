<html>
<head>
<meta http-equiv="Content-type" content="text/html; charset=UTF-8">
<title>Installation and usage : CRFSuite for variable-order Markov models</title>
</head>
<body>
<h1>Installation and usage of CRFSuite for variable-order Markov models</h1>
<h2>Installation</h2>
<p>
In order to build this software from the source code, you will need <a href="http://www.chokkan.org/software/liblbfgs/">libLBFGS</a>, as is also the case for the original CRFSuite (You don't need libLBFGS to use the Windows binary as they are statically linked. <a href="#usage">Skip to the usage</a>). Here is typical steps to take on a Unix-compatible platforms.
</p>
<p>First, download the source code package from the <a href="http://www.chokkan.org/software/liblbfgs/">libLBFGS</a> page and unpack it to a temporary directory. Execute 'configure' with a '--prefix' option to designate the install path. 'make' &amp; 'make install'.
<pre>
<code>
[/path/where/you/extracted/libLBFGS/] ./configure --prefix=/install/path/of/libLBFGS/
[/path/where/you/extracted/libLBFGS/] make
[/path/where/you/extracted/libLBFGS/] make install
</code>
</pre>
</p>
<p>
Next, download the package of CRFSuite for variable-order Markov models (crfsuite-variableorder) from the <a href="https://github.com/hiroshi-manabe/crfsuite-variableorder">github repository</a> and unpack it to a temporary directory. Execute 'configure' with a '--with-liblbfgs' to designate the libLBFGS install path above and a '--prefix' option to designate the install path for crfsuite-variableorder. 'make' &amp; 'make install'.
<pre>
<code>
[/path/where/you/extracted/crfsuite-variableorder/] ./configure --with-liblbfgs=/install/path/of/libLBFGS/ --prefix=/install/path/of/crfsuite-variableorder/
[/path/where/you/extracted/crfsuite-variableorder/] make
[/path/where/you/extracted/crfsuite-variableorder/] make install
</code>
</pre>
</p>
<a name="usage">
<h2>Usage</h2>
<p>
The basic usage is common as the original <a href="http://www.chokkan.org/software/crfsuite/">CRFSuite</a>. The difference lies that crfsuite-variableorder does not auto-generate features and instead read them from a file, which should be specified by -f or --features option. Here is the format to describe a featre of (n-1)th order.
<pre>
ATTR[tab]LABEL0[tab]LABEL1[tab]...LABELn[newline]
</pre>
The order is : LABEL0 for the current position, LABEL1 for the previous position etc. For example, a feature activated when the label sequence to the current position is NN-VBZ-IN and the word at the current position is "like" should be described as follows:
<pre>
W0_like[tab]IN[tab]VBZ[tab]NN[newline]
</pre>
where "W0_" is an arbitrary prefix.
</p>

<p>
The format of training/evaluation data file is the same as the original CRFSuite. Please refer to the <a href="http://www.chokkan.org/software/crfsuite/manual.html">CRFSuite manual</a>.
</p>
<p>
It is not realistic to manually write the features/training/evaluation files, so you will need a script to generate them. The source and binary packages contain an "example" directory, under which you can find a python script "conv.py". I wrote this script for the English POS-tagging task. You should custamize the script in order to apply to other kind of tasks.
</p>
<p>
If you have Penn Treebank 3, you can reproduce the experiences in my thesis:
</p>
<p>
Open mrg_to_pos.py under "example" and modify the part which contains the path to the "mrg" directory. Run the script to convert the .mrg files into text files, which should look like:
<pre>
NNP	Pierre
NNP	Vinken
,	,
CD	61
NNS	years
JJ	old
,	,
...
</pre>
</p>
<p>
train.txt and test.txt respectively contain data in 0-18 and 22-24. Execute "conv.py" to generate the same dataset as in "maximun 2nd order" in my thesis (the number of the features won't be exactly the same because of the changes in end-of-sequence processing. Features/training/evaluation data should be output respectively as "features.txt", "train_data.txt", "test_data.txt". features.txt should look like as follows:
<pre>
</pre>
train_data.txt/test_data.txt should look like as follows:
<pre>
NNP	LABEL	W0_Pierre	W-1_	W+1_Vinken	W-10__Pierre	W0+1_Pierre_Vinken	W-2-1__	W-2-10___Pierre	W-3-2-1___	suf1_e	pre1_P	suf2_re	pre2_Pi	suf3_rre	pre3_Pie	suf4_erre	pre4_Pier	suf5_ierre	pre5_Pierr	suf6_Pierre	pre6_Pierre	CONTAIN_UPPER
NNP	LABEL	W0_Vinken	W-1_Pierre	W+1_,	W-10_Pierre_Vinken	W0+1_Vinken_,	W-2-1__Pierre	W-2-10__Pierre_Vinken	W-3-2-1___Pierre	suf1_n	pre1_V	suf2_en	pre2_Vi	suf3_ken	pre3_Vin	suf4_nken	pre4_Vink	suf5_inken	pre5_Vinke	suf6_Vinken	pre6_Vinken	CONTAIN_UPPER
,	LABEL	W0_,	W-1_Vinken	W+1_61	W-10_Vinken_,	W0+1_,_61	W-2-1_Pierre_Vinken	W-2-10_Pierre_Vinken_,	W-3-2-1__Pierre_Vinken	suf1_,	pre1_,
</pre>
</p>
<p>
To train the model, input the following line:
<pre>
<code>
$ crfsuite learn -m wsj.model -t test_data.txt -f features.txt train_data.txt
</code>
</pre>
The program consumes 20 to 30 gigabytes (it could have been programmed to swap out the data as it is accessed only sequentially, but I choosed not to, because of the lack of time). The model will be output as wsj.model.
</p>
<p>
To tag sentences using the model, input the following line:
<pre>
<code>
$ crfsuite tag -m wsj.model test.txt
</code>
</pre>
</p>
<p>
To evaluate the tagging performance, input to
<pre>
<code>
$ crfsuite tag -m wsj.model -qt test.txt
</code>
</pre>
</p>
<h2>Detailed usage</h2>
<p>
CRFSuite for variable-order Markov model shares most of the options with the original CRFSuite. Please refer to the <a href="http://www.chokkan.org/software/crfsuite/manual.html">CRFSuite manual</a>.
</p>
<p>
Here is the list of the differences.
</p>
<ul>
<li>As stated above, you must provide a "-f" or "--features" option to specify the file to read features from when the "learn" option is selected.</li>
<li>All parameters of the original CRFSuite that have to do with the features are disabled, including "feature.minfreq". To achieve the same goal, you have to count the appearances of each feature in your conversion script and exclude the ones that don't match the criteria. The appearances of features listed in the features file will be computed automatically and used for learning.
</li>
<li>Training by SGD is not supported. Consequently, the only option that can be specified by "algorithm" option is "lbfgs".
</li>
<li>The "dump" option is not implemented yet.</li>
</ul>
</body>
</html>
